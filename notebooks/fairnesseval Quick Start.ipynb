{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# fairnesseval API\n","Note: you can run **[this notebook live in Google Colab](https://colab.research.google.com/github/softlab-unimore/fairnesseval/blob/main/notebooks/fairnesseval%20Quick%20Start.ipynb)**."],"metadata":{"id":"-D3BoNEC_YSD"}},{"cell_type":"code","source":["!pip install git+https://github.com/softlab-unimore/fairnesseval@main"],"metadata":{"id":"4nCp2TnDIWDN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset download\n","The following code will download the datasets following the instructions in aif360 errors.\n","It should be changed according to your paths (python path especially)."],"metadata":{"id":"L0Zlk_08JgHU"}},{"cell_type":"code","source":["# @title Code to download datasets in colab\n","import requests\n","import os\n","\n","url_list = [\"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\",\n","            'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data',\n","\t'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc',\n","    'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',\n","\t'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',\n","\t'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names',\n","            ]\n","compas = \"/usr/local/lib/python3.10/dist-packages/aif360/data/raw/compas\"\n","german = \"/usr/local/lib/python3.10/dist-packages/aif360/data/raw/german\"\n","adult =  \"/usr/local/lib/python3.10/dist-packages/aif360/data/raw/adult\"\n","save_path_list = [compas] + [german] *2 + [adult] * 3\n","\n","for url, save_path in zip(url_list, save_path_list):\n","    os.makedirs(save_path, exist_ok=True)\n","    full_path = os.path.join(save_path, url.split('/')[-1])\n","    response = requests.get(url)\n","    with open(full_path, 'wb') as file:\n","        file.write(response.content)\n","\n","import os\n","os.makedirs('cached_data/2018/1-Year', exist_ok=True)\n"],"metadata":{"id":"d4Vs-t3QDXD2","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## You need to restart the kernel"],"metadata":{"id":"GoWS3XOaAV45"}},{"cell_type":"code","source":["print(\"üîÅ Restarting kernel...\")\n","# get_ipython().kernel.do_shutdown(True)\n","quit()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"We0GzNtdte3I","executionInfo":{"status":"ok","timestamp":1713356470736,"user_tz":-120,"elapsed":5,"user":{"displayName":"Andrea Baraldi","userId":"09082697304872948476"}},"outputId":"008b40c6-ab4f-4a5b-8c03-9cc923e0146a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÅ Restarting kernel...\n"]}]},{"cell_type":"markdown","source":["# fairnesseval API\n","This tool provides two interfaces for running fairness experiments on your data.\n","\n","**1. Python Interface**\n","You can define the experiment settings in the form of a Python dictionary and use one of the following Python functions to run experiments:\n","    \n","1.1. `fairnesseval.run.launch_experiment_by_id` let you define and organize your experiments in a python module (default at `fairnesseval.experiment_definitions`). Then you will need to call this function by specifying only the id of the experiment you want to run. **This is the reccommended interface.**\n","    \n","1.2. `fairnesseval.run.launch_experiment_by_config` let you run an experiment by passing the dictionary of parameters of your experiment in input.\n","\n","**2. Command Line Interface**\n","Alternatively, you can use the command line interface of `fairnesseval.run` to specify the experiment settings using traditional CLI parameters."],"metadata":{"id":"ERXd-AUb2Met"}},{"cell_type":"markdown","source":["## 1 Python Interface"],"metadata":{"id":"WcWIJmEn2Qha"}},{"cell_type":"markdown","source":["To launch an experiment you can run Python script that read experiment parameters from a module (default at `fairnesseval.experiment_definitions`).\n","\n","Loading experiment definitions is more powerful and flexible, it allows to:\n","\n","*   launch multiple experiments in a row.\n","*   specify multiple datasets.\n","*   specify multiple models.\n","*   configurations are more organized and readable.\n","*   have additional logging.\n","\n","\n"],"metadata":{"id":"gn5NRU1p2hob"}},{"cell_type":"markdown","source":["### TODO: Define your experiment in a file.\n","(You can find example of experiment configuration in `fairnesseval.experiment_definitions`).\n","\n","Eg.: Create `exp_def.py` and define an experiment.\n","```python\n","import itertools\n","import json\n","\n","import pandas as pd\n","\n","RANDOM_SEEDs_RESTRICTED_V1 = [1]\n","\n","TRAIN_FRACTIONS_SMALLER_DATASETS_v1 = [0.063, 0.251, 1.]\n","TRAIN_FRACTIONS_v1 = [0.001, 0.004, 0.016, 0.063, 0.251, 1]  # np.geomspace(0.001,1,7) np.linspace(0.001,1,7)\n","\n","experiment_definitions = [\n","    {\n","        'experiment_id': 'new_experiment',\n","        'dataset_names': ('adult_sigmod',),\n","        'model_names': ('LogisticRegression',),\n","        'random_seeds': RANDOM_SEEDs_RESTRICTED_V1,\n","        'results_path': './demo_results'\n","    }\n","]\n","\n","```\n","\n"],"metadata":{"id":"G_ylbpALJEml"}},{"cell_type":"markdown","source":["### Run the experiment\n","Copy the path to the experiment configuration file just defined.\n","\n","In my case: `/content/exp_def.py`\n","\n","Then run the experiment in Colab\n","\n"],"metadata":{"id":"ZllCbAb1M1IA"}},{"cell_type":"code","source":["import fairnesseval as fe\n","fe.run.launch_experiment_by_id('new_experiment', '/content/exp_def.py')"],"metadata":{"id":"X1BhG4l2OhRg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","or save the following code in a .py file to run the experiments.\n","\n","\n","```python\n","# FILE runner.py\n","import fairnesseval as fe\n","\n","if __name__ == \"__main__\":\n","    conf_todo = [\n","        \"new_experiment\",\n","        # ... (list of configurations to be executed)\n","    ]\n","    for x in conf_todo:\n","        fe.run.launch_experiment_by_id(x, '/content/exp_def.py')\n","\n","```\n","\n","Then launch the python script"],"metadata":{"id":"6TqlxFcPOhjx"}},{"cell_type":"code","source":["!python -m runner"],"metadata":{"id":"yjh9QpMb9kum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Otherwise you can use `launch_experiment_by_config`.\n","E.g.:"],"metadata":{"id":"hRokI3ObADED"}},{"cell_type":"code","source":["fe.run.launch_experiment_by_config(\n","    {\n","        'experiment_id': 'new_experiment',\n","        'dataset_names': ('adult_sigmod',),\n","        'model_names': ('LogisticRegression',),\n","        'random_seeds': [1],\n","        'results_path': './demo_results'\n","    }\n","    )"],"metadata":{"id":"VfYrY0e9DpEe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CLI interface\n","The equivalent CLI call to run the experiment defined before is:"],"metadata":{"id":"NlhpcpCTQQda"}},{"cell_type":"code","source":["!python -m fairnesseval.run --dataset_name adult_sigmod --model_name LogisticRegression --experiment_id new_experiment --random_seeds 1 --results_path /content/demo_results"],"metadata":{"id":"kyf4gRHoQZnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_JVjlhrfCuZJ"},"execution_count":null,"outputs":[]}]}